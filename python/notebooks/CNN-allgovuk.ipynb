{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional NN to classify all govuk content to level2 taxons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on:\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load requirements and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Input, Dense, Activation, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import rmsprop\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matthewupson/.pyenv/shims/python3\r\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.9 s, sys: 6.65 s, total: 52.6 s\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%time content = pd.read_csv('../../data/clean_content.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.9 ms, sys: 11.5 ms, total: 33.4 ms\n",
      "Wall time: 44.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time taxons = pd.read_csv('../../data/clean_taxons.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ------------- This data prep step should move to clean_content.py -------------\n",
    "\n",
    "Remove taxons that are legacy (Imported), World, Corporate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxons shape after deleting imported top taxons: (2027, 7)\n",
      "Taxons shape after deleting 'World' top taxons: (2027, 7)\n",
      "Taxons shape after deleting 'corporate information' top taxons: (2027, 7)\n",
      "Taxons shape after converting nans to Nones: (2027, 7)\n"
     ]
    }
   ],
   "source": [
    "taxons = taxons[['base_path','content_id','taxon_name','level1taxon','level2taxon','level3taxon','level4taxon']]\n",
    "\n",
    "taxons['level1taxon'] = taxons['level1taxon'].astype('str')\n",
    "\n",
    "# Drop taxons that start with Imported (i.e. legacy taxons at the top level)\n",
    "\n",
    "taxons = taxons[~taxons['level1taxon'].str.startswith(\"Imported\")]\n",
    "taxons = taxons[~taxons['taxon_name'].str.startswith(\"Imported\")]\n",
    "\n",
    "print(\"Taxons shape after deleting imported top taxons: {}\".format(taxons.shape))\n",
    "\n",
    "taxons = taxons[taxons.level1taxon != 'World']\n",
    "taxons = taxons[taxons.taxon_name != 'World']\n",
    "\n",
    "print(\"Taxons shape after deleting 'World' top taxons: {}\".format(taxons.shape))\n",
    "\n",
    "taxons = taxons[taxons.level1taxon != 'Corporate information']\n",
    "taxons = taxons[taxons.taxon_name != 'Corporate information']\n",
    "print(\"Taxons shape after deleting 'corporate information' top taxons: {}\".format(taxons.shape))\n",
    "\n",
    "# Convert nans to None\n",
    "\n",
    "taxons['level1taxon'] = taxons['level1taxon'].where(taxons['level1taxon'] != 'nan', None)\n",
    "taxons['level2taxon'] = taxons['level2taxon'].where(~taxons['level2taxon'].isnull(), None)\n",
    "\n",
    "print(\"Taxons shape after converting nans to Nones: {}\".format(taxons.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_path</th>\n",
       "      <th>content_id</th>\n",
       "      <th>taxon_name</th>\n",
       "      <th>level1taxon</th>\n",
       "      <th>level2taxon</th>\n",
       "      <th>level3taxon</th>\n",
       "      <th>level4taxon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/business/taxon</td>\n",
       "      <td>495afdb6-47be-4df1-8b38-91c8adb1eefc</td>\n",
       "      <td>Business</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/crime-justice-and-law</td>\n",
       "      <td>ba951b09-5146-43be-87af-44075eac3ae9</td>\n",
       "      <td>Crime, justice and law</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/defence</td>\n",
       "      <td>e491505c-77ae-45b2-84be-8c94b94f6a2b</td>\n",
       "      <td>Defence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/childcare-parenting/entering-staying-uk</td>\n",
       "      <td>ba3a9702-da22-487f-86c1-8334a730e559</td>\n",
       "      <td>Entering and staying in the UK</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/environment</td>\n",
       "      <td>3cf97f69-84de-41ae-bc7b-7e2cc238fa58</td>\n",
       "      <td>Environment</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  base_path  \\\n",
       "0                           /business/taxon   \n",
       "2                    /crime-justice-and-law   \n",
       "3                                  /defence   \n",
       "4  /childcare-parenting/entering-staying-uk   \n",
       "5                              /environment   \n",
       "\n",
       "                             content_id                      taxon_name  \\\n",
       "0  495afdb6-47be-4df1-8b38-91c8adb1eefc                        Business   \n",
       "2  ba951b09-5146-43be-87af-44075eac3ae9          Crime, justice and law   \n",
       "3  e491505c-77ae-45b2-84be-8c94b94f6a2b                         Defence   \n",
       "4  ba3a9702-da22-487f-86c1-8334a730e559  Entering and staying in the UK   \n",
       "5  3cf97f69-84de-41ae-bc7b-7e2cc238fa58                     Environment   \n",
       "\n",
       "  level1taxon level2taxon level3taxon level4taxon  \n",
       "0        None        None         NaN         NaN  \n",
       "2        None        None         NaN         NaN  \n",
       "3        None        None         NaN         NaN  \n",
       "4        None        None         NaN         NaN  \n",
       "5        None        None         NaN         NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>base_path</th>\n",
       "      <th>content_id</th>\n",
       "      <th>description</th>\n",
       "      <th>details</th>\n",
       "      <th>document_type</th>\n",
       "      <th>first_published_at</th>\n",
       "      <th>locale</th>\n",
       "      <th>primary_publishing_organisation</th>\n",
       "      <th>publishing_app</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>variable</th>\n",
       "      <th>taxon_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/vehicle-tax</td>\n",
       "      <td>fa748fae-3de4-4266-ae85-0797ada3f40c</td>\n",
       "      <td>renew or tax your vehicle for the first time u...</td>\n",
       "      <td>{'will_continue_on': '', 'department_analytics...</td>\n",
       "      <td>transaction</td>\n",
       "      <td>2016-02-29T09:24:10.000+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>publisher</td>\n",
       "      <td>tax your vehicle</td>\n",
       "      <td></td>\n",
       "      <td>tax your vehicle renew or tax your vehicle for...</td>\n",
       "      <td>0</td>\n",
       "      <td>948b6dd4-45b3-45ab-a5c6-5dbce75542a6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/student-finance-register-login</td>\n",
       "      <td>e57daef4-5eb5-431c-b0ad-14119ab0355f</td>\n",
       "      <td>your student finance online account - check pa...</td>\n",
       "      <td>{'will_continue_on': 'the Student Finance Engl...</td>\n",
       "      <td>transaction</td>\n",
       "      <td>2016-02-29T09:24:10.000+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>publisher</td>\n",
       "      <td>student finance login</td>\n",
       "      <td></td>\n",
       "      <td>student finance login your student finance onl...</td>\n",
       "      <td>0</td>\n",
       "      <td>64aa6eec-48b5-481d-9131-9c8b6326eea1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>/get-information-about-a-company</td>\n",
       "      <td>9ca1a27b-af7b-44d2-b10d-0a6d0e3ff53d</td>\n",
       "      <td>get company information including registered a...</td>\n",
       "      <td>{'will_continue_on': '', 'department_analytics...</td>\n",
       "      <td>transaction</td>\n",
       "      <td>2016-02-29T09:24:10.000+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>publisher</td>\n",
       "      <td>get information about a company</td>\n",
       "      <td></td>\n",
       "      <td>get information about a company get company in...</td>\n",
       "      <td>0</td>\n",
       "      <td>a1119685-ffef-4417-a3e4-116014ad4523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>/check-vehicle-tax</td>\n",
       "      <td>0889f128-e479-465f-b3e1-a3db6a3879cf</td>\n",
       "      <td>check and report if a vehicle has up-to-date v...</td>\n",
       "      <td>{'will_continue_on': '', 'department_analytics...</td>\n",
       "      <td>transaction</td>\n",
       "      <td>2016-02-29T09:24:10.000+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>publisher</td>\n",
       "      <td>check if a vehicle is taxed</td>\n",
       "      <td></td>\n",
       "      <td>check if a vehicle is taxed check and report i...</td>\n",
       "      <td>0</td>\n",
       "      <td>948b6dd4-45b3-45ab-a5c6-5dbce75542a6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>/jobsearch</td>\n",
       "      <td>a01fcb59-5dc8-4bf5-b06d-dd567a6d2f5f</td>\n",
       "      <td>find a job using the universal jobmatch servic...</td>\n",
       "      <td>{'will_continue_on': 'Universal Jobmatch', 'de...</td>\n",
       "      <td>transaction</td>\n",
       "      <td>2016-02-29T09:24:10.000+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>publisher</td>\n",
       "      <td>find a job with universal jobmatch</td>\n",
       "      <td></td>\n",
       "      <td>find a job with universal jobmatch find a job ...</td>\n",
       "      <td>0</td>\n",
       "      <td>21bfd8f6-3360-43f9-be42-b00002982d70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                         base_path  \\\n",
       "0           0                      /vehicle-tax   \n",
       "1           1   /student-finance-register-login   \n",
       "2           3  /get-information-about-a-company   \n",
       "3           4                /check-vehicle-tax   \n",
       "4           5                        /jobsearch   \n",
       "\n",
       "                             content_id  \\\n",
       "0  fa748fae-3de4-4266-ae85-0797ada3f40c   \n",
       "1  e57daef4-5eb5-431c-b0ad-14119ab0355f   \n",
       "2  9ca1a27b-af7b-44d2-b10d-0a6d0e3ff53d   \n",
       "3  0889f128-e479-465f-b3e1-a3db6a3879cf   \n",
       "4  a01fcb59-5dc8-4bf5-b06d-dd567a6d2f5f   \n",
       "\n",
       "                                         description  \\\n",
       "0  renew or tax your vehicle for the first time u...   \n",
       "1  your student finance online account - check pa...   \n",
       "2  get company information including registered a...   \n",
       "3  check and report if a vehicle has up-to-date v...   \n",
       "4  find a job using the universal jobmatch servic...   \n",
       "\n",
       "                                             details document_type  \\\n",
       "0  {'will_continue_on': '', 'department_analytics...   transaction   \n",
       "1  {'will_continue_on': 'the Student Finance Engl...   transaction   \n",
       "2  {'will_continue_on': '', 'department_analytics...   transaction   \n",
       "3  {'will_continue_on': '', 'department_analytics...   transaction   \n",
       "4  {'will_continue_on': 'Universal Jobmatch', 'de...   transaction   \n",
       "\n",
       "              first_published_at locale primary_publishing_organisation  \\\n",
       "0  2016-02-29T09:24:10.000+00:00     en                             NaN   \n",
       "1  2016-02-29T09:24:10.000+00:00     en                             NaN   \n",
       "2  2016-02-29T09:24:10.000+00:00     en                             NaN   \n",
       "3  2016-02-29T09:24:10.000+00:00     en                             NaN   \n",
       "4  2016-02-29T09:24:10.000+00:00     en                             NaN   \n",
       "\n",
       "  publishing_app                               title body  \\\n",
       "0      publisher                    tax your vehicle        \n",
       "1      publisher               student finance login        \n",
       "2      publisher     get information about a company        \n",
       "3      publisher         check if a vehicle is taxed        \n",
       "4      publisher  find a job with universal jobmatch        \n",
       "\n",
       "                                       combined_text  variable  \\\n",
       "0  tax your vehicle renew or tax your vehicle for...         0   \n",
       "1  student finance login your student finance onl...         0   \n",
       "2  get information about a company get company in...         0   \n",
       "3  check if a vehicle is taxed check and report i...         0   \n",
       "4  find a job with universal jobmatch find a job ...         0   \n",
       "\n",
       "                               taxon_id  \n",
       "0  948b6dd4-45b3-45ab-a5c6-5dbce75542a6  \n",
       "1  64aa6eec-48b5-481d-9131-9c8b6326eea1  \n",
       "2  a1119685-ffef-4417-a3e4-116014ad4523  \n",
       "3  948b6dd4-45b3-45ab-a5c6-5dbce75542a6  \n",
       "4  21bfd8f6-3360-43f9-be42-b00002982d70  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the taxons with the content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 s, sys: 86.3 ms, total: 1.09 s\n",
      "Wall time: 1.09 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(335720, 23)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time content_taxons = pd.merge(left=content, right=taxons, left_on='taxon_id', right_on='content_id', how='outer', indicator=True)\n",
    "\n",
    "assert content_taxons.shape == (335720, 23)\n",
    "content_taxons.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 266753 missing level2taxons (which we need to predict) out of a total 335720 content items.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} missing level2taxons (which we need to predict) out of a total {} content items.'\n",
    "      .format(sum(content_taxons['level2taxon'].isnull()), \n",
    "              len(content_taxons['level2taxon'].isnull())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows where there are no level2 taxons (i.e. we need to predict for them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This leaves us with 68967 pre-classified rows.\n"
     ]
    }
   ],
   "source": [
    "content_taxons.dropna(subset = ['level2taxon'], inplace=True)\n",
    "print('This leaves us with {} pre-classified rows.'.format(content_taxons.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are likely to be lots of content items that have more than one tag. Check here and remove for now:\n",
    "\n",
    "__TODO: devise a way to deal with multiple tags applied to each content item.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dupes</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>43212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dupes  count\n",
       "0       1  43212\n",
       "1       2   8871\n",
       "2       3   1614\n",
       "3       4    447\n",
       "4       5     79\n",
       "5       6     43\n",
       "6       7      8\n",
       "7       8     10\n",
       "8       9      2\n",
       "9      10      5\n",
       "10     11     33\n",
       "11     12      1\n",
       "12     13      2\n",
       "13     14      1\n",
       "14     15      1\n",
       "15     16      1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify where duplicates exist on content_id and count\n",
    "\n",
    "dupes = content_taxons['content_id_x'].value_counts().to_frame('dupes')\n",
    "dupes = dupes.groupby('dupes').size().to_frame('count')\n",
    "\n",
    "# Add index as a column\n",
    "\n",
    "dupes.reset_index(level=0, inplace=True)\n",
    "dupes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripping multiply applied tags to one will leave a total of 54330 tagged content items to train on\n"
     ]
    }
   ],
   "source": [
    "multiple_tags = sum(dupes.loc[dupes['dupes'] > 1, 'count'])\n",
    "single_tags = sum(dupes.loc[dupes['dupes'] == 1, 'count'])\n",
    "\n",
    "print('Stripping multiply applied tags to one will '\n",
    "      'leave a total of {} tagged content items to train on'\n",
    "      .format(multiple_tags + single_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deduplication that are (68967, 23) items.\n",
      "After deduplication that are (54331, 23) items.\n"
     ]
    }
   ],
   "source": [
    "print('Before deduplication that are {} items.'.format(content_taxons.shape))\n",
    "      \n",
    "content_taxons.drop_duplicates(subset = ['content_id_x'], inplace=True)\n",
    "      \n",
    "print('After deduplication that are {} items.'.format(content_taxons.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format our text samples and labels into tensors that can be fed into a neural network. To do this, we will rely on Keras utilities keras.preprocessing.text.Tokenizer and keras.preprocessing.sequence.pad_sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AttributeError: 'float' object has no attribute 'lower'` error suggests that there are `NaN`s in `content_taxons['combined_text']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140104"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_taxons['level2taxon'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "texts = list(content_taxons['combined_text'])\n",
    "labels = list(content_taxons['level2taxon']) \n",
    "#possible that this is just a unique list rather than Y array\n",
    "#labels_index = dict(zip(allcontent['toptaxon_cat'], allcontent['toptaxon_code']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_NB_WORDS\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "#EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: when using the categorical_crossentropy loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros expect for a 1 at the index corresponding to the class of the sample). In order to convert integer targets into categorical targets, you can use the Keras utility to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 µs, sys: 0 ns, total: 24 µs\n",
      "Wall time: 26.9 µs\n"
     ]
    }
   ],
   "source": [
    "%time tokenizer = Tokenizer(num_words=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-644de87d9928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.4.6/lib/python3.4/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    173\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                                                      self.split)\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.4.6/lib/python3.4/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "%time tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f81a16403e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#yield one sequence per input text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.4.6/lib/python3.4/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    173\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                                                      self.split)\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.4.6/lib/python3.4/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts) #yield one sequence per input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen= MAX_SEQUENCE_LENGTH) #MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-bb863304cbc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#yield one sequence per input text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.4.6/lib/python3.4/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    173\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                                                      self.split)\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.4.6/lib/python3.4/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtranslate_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_validationsamples: 1063\n",
      "Shape of y_train: (2481, 1000)\n",
      "Shape of y_train: (2481, 14)\n",
      "Shape of x_val: (1063, 1000)\n",
      "Shape of y_val: (1063, 14)\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.3 * data.shape[0]) #validation split\n",
    "print('nb_validationsamples:', nb_validation_samples)\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "print('Shape of y_train:', x_train.shape)\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "x_val = data[-nb_validation_samples:]\n",
    "print('Shape of x_val:', x_val.shape)\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "print('Shape of y_val:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing the Embedding layer\n",
    "compute an index mapping words ot known embeddings by parsing the data dump of pre-trained embeddings\n",
    "NB stopwords haven't been removed yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('../DATA/glove.6B', 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute embedding matrix using embedding_index dict and word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))# used 6B.100d.txt\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load this embedding matrix into an Embedding layer. Note that we set trainable=False to prevent the weights from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            100, # used 6B.100d.txt\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length= MAX_SEQUENCE_LENGTH, #MAX_SEQUENCE LENGTH\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Embedding layer should be fed sequences of integers, i.e. a 2D input of shape (samples, indices). These input sequences should be padded so that they all have the same length in a batch of input data (although an Embedding layer is capable of processing sequence of heterogenous length, if you don't pass an explicit input_length argument to the layer).\n",
    "\n",
    "All that the Embedding layer does is to map the integer inputs to the vectors found at the corresponding index in the embedding matrix, i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]. This means that the output of the Embedding layer will be a 3D tensor of shape (samples, sequence_length, embedding_dim)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a 1D convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') #MAX_SEQUENCE_LENGTH\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric values are recorded at the end of each epoch on the training dataset. If a validation dataset is also provided, then the metric recorded is also calculated for the validation dataset.\n",
    "\n",
    "All metrics are reported in verbose output and in the history object returned from calling the fit() function. In both cases, the name of the metric function is used as the key for the metric values. In the case of metrics for the validation dataset, the “val_” prefix is added to the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=1, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1063, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1063, 14)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2481, 1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2481, 14)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2481 samples, validate on 1063 samples\n",
      "Epoch 1/4\n",
      "2481/2481 [==============================] - 30s - loss: 0.2568 - acc: 0.9295 - val_loss: 0.5194 - val_acc: 0.8758\n",
      "Epoch 2/4\n",
      "2481/2481 [==============================] - 30s - loss: 0.1998 - acc: 0.9412 - val_loss: 0.5542 - val_acc: 0.8843\n",
      "Epoch 3/4\n",
      "2481/2481 [==============================] - 31s - loss: 0.1657 - acc: 0.9528 - val_loss: 0.6144 - val_acc: 0.8815\n",
      "Epoch 4/4\n",
      "2481/2481 [==============================] - 29s - loss: 0.1461 - acc: 0.9645 - val_loss: 0.7399 - val_acc: 0.8786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x123d5b6d8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=4, \n",
    "          batch_size=10, \n",
    "          callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve accuracy by training longer with some regularization mechanism (such as dropout) or by fine-tuning the Embedding layer.\n",
    "\n",
    "We can also test how well we would have performed by not using pre-trained word embeddings, but instead initializing our Embedding layer from scratch and learning its weights during training. We just need to replace our Embedding layer with the following:\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)\n",
    "                            \n",
    "After 2 epochs, this approach only gets us to 90% validation accuracy, less than what the previous model could reach in just one epoch. Our pre-trained embeddings were definitely buying us something. In general, using pre-trained embeddings is relevant for natural processing tasks were little training data is available (functionally the embeddings act as an injection of outside information which might prove useful for your model).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "govuk-legacy-taxonomy",
   "language": "python",
   "name": "govuk-legacy-taxonomy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
