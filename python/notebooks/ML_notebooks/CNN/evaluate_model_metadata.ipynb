{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score \n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import functools\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_THRESHOLD=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATADIR=os.getenv('DATADIR')\n",
    "RESULTS_DIR = os.path.join(DATADIR, \"2018-03-12\")\n",
    "RESULTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data about taxons/content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_level2 = pd.read_csv(\n",
    "    os.path.join(RESULTS_DIR, 'labelled_level2.csv.gz'),\n",
    "    dtype=object,\n",
    "    compression='gzip'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create World taxon in case any items not identified \n",
    "# through doc type in clean_content are still present\n",
    "labelled_level2.loc[labelled_level2['level1taxon'] == 'World', 'level2taxon'] = 'world_level1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating categorical variable for level2taxons from values\n",
    "labelled_level2['level2taxon'] = labelled_level2['level2taxon'].astype('category')\n",
    "\n",
    "# Add 1 because of zero-indexing to get 1-number of level2taxons as numerical targets\n",
    "labelled_level2['level2taxon_code'] = labelled_level2.level2taxon.astype('category').cat.codes + 1\n",
    "\n",
    "# create dictionary of taxon category code to string label for use in model evaluation\n",
    "labels_index = dict(zip((labelled_level2['level2taxon_code']),\n",
    "                        labelled_level2['level2taxon']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Probability and y arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_train = pd.read_csv(os.path.join(RESULTS_DIR, 'true_train_1329_1203_.csv.gz'), dtype=float, compression='gzip')\n",
    "prob_train = pd.read_csv(os.path.join(RESULTS_DIR, 'train_results_1329_1203_.csv.gz'), dtype=float, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dev = pd.read_csv(os.path.join(RESULTS_DIR, 'true_dev_1329_1203_.csv.gz'), dtype=float, compression='gzip')\n",
    "prob_dev = pd.read_csv(os.path.join(RESULTS_DIR, 'dev_results_1329_1203_.csv.gz'), dtype=float, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Metadata, content_id, x, y arrays.\n",
    "A bit redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(os.path.join(RESULTS_DIR, 'train_arrays.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = np.load(os.path.join(RESULTS_DIR, 'dev_arrays.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['content_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['content_id'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge results array with labelled_level2 values\n",
    "content_id, metadata etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_array(npz_array,val):\n",
    "    df = pd.DataFrame()\n",
    "    df[val] = npz_array[val]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = ['document_type','publishing_app']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meta_to_df(meta_vars,df):\n",
    "    for var in meta_vars:\n",
    "        print(\"Working on:\",var)\n",
    "        df[var] = df['content_id'].map(dict(zip(labelled_level2['content_id'], labelled_level2[var])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = create_df_from_array(train,'content_id')\n",
    "df_true = pd.concat([df_true, true_train], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_meta_to_df(metadata,df_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob = create_df_from_array(train,'content_id')\n",
    "df_prob = pd.concat([df_prob, prob_train], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_meta_to_df(metadata,df_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loop over results and then compute filtered (in terms of metadata) f1 micro score. \n",
    "2. F1 score shold correspond to overall taxon performance, for a specific metadata value.\n",
    "3. Correlation between metadata and F1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob[[str(i) for i in range(1,219)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_f1(meta_vars, metadata, true, probs):\n",
    "    f1_scores = {}\n",
    "    for i,meta in enumerate(meta_vars):\n",
    "\n",
    "        print(i+1,\"out of\",len(meta_vars),\":\",meta)\n",
    "        number_of_samples = probs[[str(i) for i in range(1,219)]].loc[probs[metadata]==meta].shape[0]\n",
    "\n",
    "        if number_of_samples >0:\n",
    "\n",
    "            filtered_prob_array = probs[[str(i) for i in range(1,219)]].loc[probs[metadata]==meta].values\n",
    "\n",
    "            filtered_prob_array[filtered_prob_array>=P_THRESHOLD] = 1\n",
    "            filtered_prob_array[filtered_prob_array<P_THRESHOLD] = 0\n",
    "\n",
    "            filtered_true_array = true[[str(i) for i in range(1,219)]].loc[true[metadata]==meta].values\n",
    "\n",
    "            print(\"Equal size true and pred\",(len(filtered_prob_array)==len(filtered_true_array)))\n",
    "\n",
    "            f1_m = precision_recall_fscore_support(filtered_true_array, filtered_prob_array, \n",
    "                                                   average='micro', sample_weight=None)[2]\n",
    "            f1_scores[meta] = f1_m\n",
    "\n",
    "        else:\n",
    "            print(\"Metadata value\",meta,\"from\",metadata,\"not found in set.\")\n",
    "        \n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 scores in terms of meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_lists(metadata_var):\n",
    "    print(\"Getting values for:\",metadata_var)\n",
    "    print(labelled_level2[metadata_var].nunique())\n",
    "    freq_meta_vals = labelled_level2[metadata_var].value_counts()\n",
    "    print(\"Frequency dict:\",[(k,v) for k,v in freq_meta_vals.items()][0:5])\n",
    "    meta_vals = labelled_level2[metadata_var].unique()\n",
    "    print(\"Value array:\",meta_vals[0:4])\n",
    "    \n",
    "    return freq_meta_vals,meta_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,doc_types = get_meta_lists(\"document_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train_doc_type = filtered_f1(doc_types, \"document_type\", df_true, df_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_f1 = sorted(f1_scores.items(), key=operator.itemgetter(1))\n",
    "sorted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev set metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_dev = create_df_from_array(dev,'content_id')\n",
    "df_true_dev = pd.concat([df_true_dev, true_dev], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_meta_to_df(metadata,df_true_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_dev['content_id'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_level2.loc[labelled_level2['content_id']==\"33582c0d-57a3-4dc3-a601-6ef316d997af\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob_dev = create_df_from_array(dev,'content_id')\n",
    "df_prob_dev = pd.concat([df_prob_dev, prob_dev], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_meta_to_df(metadata,df_prob_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_doc, doc_list = get_meta_lists(\"document_type\")\n",
    "freq_pub_app, pub_app_list = get_meta_lists(\"publishing_app\")\n",
    "\n",
    "print(doc_list[0:2],pub_app_list[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_dev_doc = filtered_f1(doc_list, \"document_type\", df_true_dev, df_prob_dev)\n",
    "f1_dev_pub = filtered_f1(pub_app_list, \"publishing_app\", df_true_dev, df_prob_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(f1_dev_doc.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_doc = pd.DataFrame.from_dict(data=f1_dev_doc,orient='index').rename(columns={0:'F1_micro'}).reset_index().\\\n",
    "                     rename(columns={'index':'document_type',0:'F1_micro'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_supp = create_df_from_array(train,'content_id')\n",
    "add_meta_to_df(metadata,training_supp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_doc = training_supp['document_type'].value_counts()\n",
    "support_pub = training_supp['publishing_app'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_doc['support_doc_type'] = results_doc['document_type'].map(lambda x: support_doc[x] if x in support_doc else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_doc['F1_micro'].sort_values().plot( kind = 'barh', figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_doc['support_doc_type'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normal values\n",
    "\n",
    "fig = plt.figure(figsize=(30, 20))\n",
    "axes = plt.subplot()\n",
    "axes.set_xlim([0,21000])\n",
    "axes.set_ylim([0,1.1])\n",
    "\n",
    "plt.xlabel('Support Per Document Type')\n",
    "plt.ylabel('F1 micro')\n",
    "plt.scatter(results_doc['support_doc_type'][results_doc['F1_micro']>=0.8], \n",
    "            results_doc['F1_micro'][results_doc['F1_micro']>=0.8], \n",
    "            c = 'b', \n",
    "            alpha=.4)\n",
    "# less than 0.8\n",
    "plt.scatter(results_doc['support_doc_type'][results_doc['F1_micro']<0.8], \n",
    "            results_doc['F1_micro'][results_doc['F1_micro']<0.8], \n",
    "            c = 'grey', \n",
    "            alpha=.4)\n",
    "\n",
    "plt.scatter(results_doc['support_doc_type'][results_doc['F1_micro']==0], \n",
    "            results_doc['F1_micro'][results_doc['F1_micro']==0], \n",
    "            c = 'red', \n",
    "            alpha=.4)\n",
    "\n",
    "plt.scatter(results_doc['support_doc_type'][results_doc['F1_micro']==1], \n",
    "            results_doc['F1_micro'][results_doc['F1_micro']==1], \n",
    "            c = 'yellow', \n",
    "            alpha=.4)\n",
    "\n",
    "\n",
    "for label, x, y in zip(results_doc['document_type'], results_doc['support_doc_type'], results_doc['F1_micro']):\n",
    "    if y < 1:\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y), xytext=(-1, 1),\n",
    "            textcoords='offset points', ha='right', va='bottom')\n",
    "    if (y==1):\n",
    "        label = \"\"\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y), xytext=(-1, 1),\n",
    "            textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "plt.yticks(np.arange(0, 1, 0.1))        \n",
    "plt.xticks(np.arange(0, 22000, 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(RESULTS_DIR, 'predictions_meta.csv.gz'), dtype=object, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions[['content_id', 'prob', 'taxon2label', 'base_path', 'title', 'description',\n",
    "       'combined_text', 'document_type', 'first_published_at', 'primary_publishing_organisation', 'publishing_app']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[200:250].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxons = predictions['taxon2label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = pd.DataFrame(columns = predictions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for taxon in taxons:\n",
    "    taxon_spec = predictions.loc[predictions['taxon2label']==taxon]\n",
    "    sample_size = int(round(taxon_spec.shape[0]*0.1))\n",
    "    if not sample_size == 0 and taxon_spec.shape[0] > sample_size:\n",
    "        print(taxon,\": SAMPLING AT:\",sample_size)\n",
    "        subsample = subsample.append(taxon_spec.sample(n=sample_size), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample[['taxon2label','title','combined_text','base_path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample[['taxon2label','title','combined_text','base_path']].to_csv(os.path.join(RESULTS_DIR,\"subsampled_predictions_10percent.csv\"),index=False)"
   ]
  }
 ],
 "metadata": {
  "keep_output": true,
  "kernelspec": {
   "display_name": "taxon-sl",
   "language": "python",
   "name": "taxon-sl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
