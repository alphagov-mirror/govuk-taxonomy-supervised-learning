{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional NN to classify govuk content to level2 taxons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on:\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "- Consider grouping very small classes (especially if too small for evaluation)\n",
    "- Split data into training, validation and test to avoid overfitting validation data during hyperparamter searches & model architecture changes\n",
    "- Try learning embeddings\n",
    "- Try changing pos_ratio\n",
    "- Try implementing class_weights during model fit (does this do the same as the weighted binary corss entropy?)\n",
    "- Work on tensorboard callbacks\n",
    "- Create dictionary of class indices to taxon names for viewing results\n",
    "- Check model architecture\n",
    "- consider relationship of training error to validation error - overfitting/bias?\n",
    "- train longer\n",
    "- Try differnet max_sequence_length\n",
    "- Check batch size is appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load requirements and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical, layer_utils, plot_model\n",
    "\n",
    "from keras.layers import (Embedding, Input, Dense, \n",
    "                          Activation, Conv1D, MaxPooling1D, Flatten)\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import rmsprop\n",
    "from keras.callbacks import TensorBoard, Callback\n",
    "import keras.backend as K\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score \n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import functools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envoronmental vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR=os.getenv('DATADIR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_NB_WORDS\n",
    "MAX_SEQUENCE_LENGTH =1000\n",
    "#EMBEDDING_DIM\n",
    "P_THRESHOLD = 0.5 #Threshold for probability of being assigned to class\n",
    "POS_RATIO = 0.5 #ratio of positive to negative for each class in weighted binary cross entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data\n",
    "Content items tagged to level 2 taxons or lower in the topic taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_level2 = pd.read_csv(os.path.join(DATADIR, 'labelled_level2.csv'), dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create target/Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: when using the categorical_crossentropy loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros expect for a 1 at the index corresponding to the class of the sample). In order to convert integer targets into categorical targets, you can use the Keras utility to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COLLAPSE World level2taxons\n",
    "labelled_level2.loc[labelled_level2['level1taxon'] == 'World', 'level2taxon'] = 'world_level1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_level2['level2taxon'] = labelled_level2['level2taxon'].astype('category')\n",
    "\n",
    "labels = labelled_level2['level2taxon'].cat.codes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multilabel learning, the joint set of binary classification tasks is expressed with label binary indicator array: each sample is one row of a 2d array of shape (n_samples, n_classes) with binary values:  \n",
    "the one, i.e. the non zero elements, corresponds to the subset of labels.  \n",
    "An array such as np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]]) represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample.  \n",
    "Producing multilabel data as a list of sets of labels may be more intuitive. The MultiLabelBinarizer transformer can be used to convert between a collection of collections of labels and the indicator format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  first reshape wide to get columns for each level2taxon and row number = number unique urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "#get a smaller copy of data for pivoting ease (think you can work from full data actually and other cols get droopedauto)\n",
    "\n",
    "level2_reduced = labelled_level2[['content_id', 'level2taxon', 'combined_text']].copy()\n",
    "\n",
    "#how many level2taxons are there?\n",
    "print(level2_reduced.level2taxon.nunique())\n",
    "\n",
    "#count the number of taxons per content item into new column\n",
    "level2_reduced['num_taxon_per_content'] = level2_reduced.groupby([\"content_id\"])['content_id'].transform(\"count\")\n",
    "\n",
    "#Add 1 because of zero-indexing to get 1-number of level2taxons as numerical targets\n",
    "level2_reduced['level2taxon_code'] = level2_reduced.level2taxon.astype('category').cat.codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173560, 5)\n",
      "(114048, 210)\n"
     ]
    }
   ],
   "source": [
    "#reshape to wide per taxon and keep the combined text so indexing is consistent when splitting X from Y\n",
    "\n",
    "multilabel = (level2_reduced.pivot_table(index=['content_id', 'combined_text'], \n",
    "                  columns='level2taxon_code', \n",
    "                  values='num_taxon_per_content'))\n",
    "print(level2_reduced.shape)\n",
    "print(multilabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the number_of_taxons_per_content values to 1, meaning there was an entry for this taxon and this content_id, 0 otherwise\n",
    "binary_multilabel = multilabel.notnull().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114048, 210)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#will convert columns to an array of shape\n",
    "binary_multilabel[list(binary_multilabel.columns)].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert columns to an array. Each row represents a content item, each column an individual taxon\n",
    "binary_multilabel = binary_multilabel[list(binary_multilabel.columns)].values\n",
    "binary_multilabel[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(binary_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlb = MultiLabelBinarizer()\n",
    "# y = mlb.fit_transform(binary_multilabel)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this for singlelabel problems\n",
    "# labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "# print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create language data/X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format our text samples and labels into tensors that can be fed into a neural network. To do this, we will rely on Keras utilities keras.preprocessing.text.Tokenizer and keras.preprocessing.sequence.pad_sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114048,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel.index.names\n",
    "texts = multilabel.index.get_level_values('combined_text')\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 213132 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) #yield one sequence per input text\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen= MAX_SEQUENCE_LENGTH) #MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (114048, 210)\n",
      "Shape of data tensor: (114048, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of label tensor:', binary_multilabel.shape)\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_validationsamples: 22809\n",
      "number of training examples = 91239\n",
      "Shape of x_train: (91239, 1000)\n",
      "Shape of y_train: (91239, 210)\n",
      "Shape of x_val: (22809, 1000)\n",
      "Shape of y_val: (22809, 210)\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = binary_multilabel[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0]) #validation split\n",
    "print('nb_validationsamples:', nb_validation_samples)\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "print (\"number of training examples = \" + str(x_train.shape[0]))\n",
    "\n",
    "print('Shape of x_train:', x_train.shape)\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "x_val = data[-nb_validation_samples:]\n",
    "print('Shape of x_val:', x_val.shape)\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "print('Shape of y_val:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing the Embedding layer\n",
    "compute an index mapping words ot known embeddings by parsing the data dump of pre-trained embeddings\n",
    "NB stopwords haven't been removed yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(DATADIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute embedding matrix using embedding_index dict and word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))# used 6B.100d.txt\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load this embedding matrix into an Embedding layer. Note that we set trainable=False to prevent the weights from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            100, # used 6B.100d.txt\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length= MAX_SEQUENCE_LENGTH, #MAX_SEQUENCE LENGTH\n",
    "                            trainable=False)\n",
    "\n",
    "# embedding_layer = Embedding(len(word_index) + 1, \n",
    "#                             EMBEDDING_DIM, \n",
    "#                             input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Embedding layer should be fed sequences of integers, i.e. a 2D input of shape (samples, indices). These input sequences should be padded so that they all have the same length in a batch of input data (although an Embedding layer is capable of processing sequence of heterogenous length, if you don't pass an explicit input_length argument to the layer).\n",
    "\n",
    "All that the Embedding layer does is to map the integer inputs to the vectors found at the corresponding index in the embedding matrix, i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]. This means that the output of the Embedding layer will be a 3D tensor of shape (samples, sequence_length, embedding_dim)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate class weights for unbalanced datasets.\n",
    "paramter to model.fit = __class_weight__: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n",
    "\n",
    "Implement class_weight from sklearn:\n",
    "\n",
    "- Import the module \n",
    "\n",
    "`from sklearn.utils import class_weight`\n",
    "- calculate the class weight, If ‘balanced’, class weights will be given by n_samples / (n_classes * np.bincount(y)):\n",
    "\n",
    "`class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)`\n",
    "\n",
    "- change it to a dict in order to work with Keras.\n",
    "\n",
    "`class_weight_dict = dict(enumerate(class_weight))`\n",
    "\n",
    "- Add to model fitting\n",
    "\n",
    "`model.fit(X_train, y_train, class_weight=class_weight)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c7dcc0363061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclass_weight_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tag_tax/govuk-taxonomy-supervised-learning/tax_SL/lib/python3.4/site-packages/sklearn/utils/class_weight.py\u001b[0m in \u001b[0;36mcompute_class_weight\u001b[0;34m(class_weight, classes, y)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         raise ValueError(\"classes should include all valid labels that can \"\n\u001b[1;32m     43\u001b[0m                          \"be in y\")\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weight_dict = dict(enumerate(class_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.00756\n",
      "8.01512\n"
     ]
    }
   ],
   "source": [
    "class WeightedBinaryCrossEntropy(object):\n",
    "\n",
    "    def __init__(self, pos_ratio):\n",
    "        neg_ratio = 1. - pos_ratio\n",
    "        #self.pos_ratio = tf.constant(pos_ratio, tf.float32)\n",
    "        self.pos_ratio = pos_ratio\n",
    "        #self.weights = tf.constant(neg_ratio / pos_ratio, tf.float32)\n",
    "        self.weights = neg_ratio / pos_ratio\n",
    "        self.__name__ = \"weighted_binary_crossentropy({0})\".format(pos_ratio)\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.weighted_binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    def weighted_binary_crossentropy(self, y_true, y_pred):\n",
    "            # Transform to logits\n",
    "            epsilon = tf.convert_to_tensor(K.common._EPSILON, y_pred.dtype.base_dtype)\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
    "            y_pred = tf.log(y_pred / (1 - y_pred))\n",
    "\n",
    "            cost = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, self.weights)\n",
    "            return K.mean(cost * self.pos_ratio, axis=-1)\n",
    "    \n",
    "y_true_arr = np.array([0,1,0,1], dtype=\"float32\")\n",
    "y_pred_arr = np.array([0,0,1,1], dtype=\"float32\")\n",
    "y_true = tf.constant(y_true_arr)\n",
    "y_pred = tf.constant(y_pred_arr)\n",
    "\n",
    "with tf.Session().as_default(): \n",
    "    print(WeightedBinaryCrossEntropy(0.5)(y_true, y_pred).eval())\n",
    "    print(binary_crossentropy(y_true, y_pred).eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### difficulty getting global precision/recall metrics . CAUTION interpreting monitoring metrics\n",
    "fcholltet: \"Basically these are all global metrics that were approximated\n",
    "batch-wise, which is more misleading than helpful. This was mentioned in\n",
    "the docs but it's much cleaner to remove them altogether. It was a mistake\n",
    "to merge them in the first place.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcor(y_true, y_pred):\n",
    "     #matthews_correlation\n",
    "     y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "     y_pred_neg = 1 - y_pred_pos\n",
    " \n",
    " \n",
    "     y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "     y_neg = 1 - y_pos\n",
    " \n",
    " \n",
    "     tp = K.sum(y_pos * y_pred_pos)\n",
    "     tn = K.sum(y_neg * y_pred_neg)\n",
    " \n",
    " \n",
    "     fp = K.sum(y_neg * y_pred_pos)\n",
    "     fn = K.sum(y_pos * y_pred_neg)\n",
    " \n",
    " \n",
    "     numerator = (tp * tn - fp * fn)\n",
    "     denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    " \n",
    " \n",
    "     return numerator / (denominator + K.epsilon())\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a 1D convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASSES = y_train.shape[1]\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') #MAX_SEQUENCE_LENGTH\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu', name = 'conv0')(embedded_sequences)\n",
    "\n",
    "x = MaxPooling1D(5, name = 'max_pool0')(x)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu', name = 'conv1')(x)\n",
    "\n",
    "x = MaxPooling1D(5 , name = 'max_pool1')(x)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu', name = 'conv2')(x)\n",
    "\n",
    "x = MaxPooling1D(35, name = 'global_max_pool')(x)  # global max pooling\n",
    "\n",
    "x = Flatten()(x) #reduce dimensions from 3 to 2; convert to vector + FULLYCONNECTED\n",
    "\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "x = Dense(NB_CLASSES, activation='sigmoid', name = 'fully_connected')(x)\n",
    "\n",
    "# Create model. \n",
    "# This creates Keras model instance, will use this instance to train/test the model.\n",
    "model = Model(sequence_input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=WeightedBinaryCrossEntropy(POS_RATIO),\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['binary_accuracy', precision, recall, f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric values are recorded at the end of each epoch on the training dataset. If a validation dataset is also provided, then the metric recorded is also calculated for the validation dataset.\n",
    "\n",
    "All metrics are reported in verbose output and in the history object returned from calling the fit() function. In both cases, the name of the metric function is used as the key for the metric values. In the case of metrics for the validation dataset, the “val_” prefix is added to the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now built a function to describe your model. To train and test this model, there are four steps in Keras:\n",
    "1. Create the model by calling the function above\n",
    "2. Compile the model by calling `model.compile(optimizer = \"...\", loss = \"...\", metrics = [\"accuracy\"])`\n",
    "3. Train the model on train data by calling `model.fit(x = ..., y = ..., epochs = ..., batch_size = ...)`\n",
    "4. Test the model on test data by calling `model.evaluate(x = ..., y = ...)`\n",
    "\n",
    "If you want to know more about `model.compile()`, `model.fit()`, `model.evaluate()` and their arguments, refer to the official [Keras documentation](https://keras.io/models/model/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         21313300  \n",
      "_________________________________________________________________\n",
      "conv0 (Conv1D)               (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pool0 (MaxPooling1D)     (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pool1 (MaxPooling1D)     (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv1D)               (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pool (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      (None, 210)               27090     \n",
      "=================================================================\n",
      "Total params: 21,585,126\n",
      "Trainable params: 271,826\n",
      "Non-trainable params: 21,313,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard callbacks /metrics /monitor training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Size of these files is killing storage during training. Is it histograms?**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TensorBoard(log_dir='./multilabel_logs', histogram_freq=1, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
    "        val_targ = self.model.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\"- val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    " \n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91239 samples, validate on 22809 samples\n",
      "Epoch 1/10\n",
      "91239/91239 [==============================] - 1101s 12ms/step - loss: 0.0157 - binary_accuracy: 0.9917 - precision: 0.5189 - recall: 0.1002 - f1: nan - val_loss: 0.0101 - val_binary_accuracy: 0.9941 - val_precision: 0.7801 - val_recall: 0.2274 - val_f1: 0.3510\n",
      "Epoch 2/10\n",
      "91239/91239 [==============================] - 1089s 12ms/step - loss: 0.0088 - binary_accuracy: 0.9947 - precision: 0.7628 - recall: 0.3454 - f1: 0.4722 - val_loss: 0.0077 - val_binary_accuracy: 0.9952 - val_precision: 0.8065 - val_recall: 0.4118 - val_f1: 0.5442\n",
      "Epoch 3/10\n",
      "91239/91239 [==============================] - 1081s 12ms/step - loss: 0.0071 - binary_accuracy: 0.9954 - precision: 0.8003 - recall: 0.4678 - f1: 0.5891 - val_loss: 0.0072 - val_binary_accuracy: 0.9955 - val_precision: 0.8071 - val_recall: 0.4754 - val_f1: 0.5974\n",
      "Epoch 4/10\n",
      "91239/91239 [==============================] - 1081s 12ms/step - loss: 0.0063 - binary_accuracy: 0.9959 - precision: 0.8174 - recall: 0.5289 - f1: 0.6412 - val_loss: 0.0067 - val_binary_accuracy: 0.9958 - val_precision: 0.8247 - val_recall: 0.5165 - val_f1: 0.6344\n",
      "Epoch 5/10\n",
      "91239/91239 [==============================] - 1087s 12ms/step - loss: 0.0058 - binary_accuracy: 0.9961 - precision: 0.8276 - recall: 0.5678 - f1: 0.6725 - val_loss: 0.0066 - val_binary_accuracy: 0.9959 - val_precision: 0.8096 - val_recall: 0.5409 - val_f1: 0.6478\n",
      "Epoch 6/10\n",
      "91239/91239 [==============================] - 1090s 12ms/step - loss: 0.0054 - binary_accuracy: 0.9964 - precision: 0.8349 - recall: 0.5996 - f1: 0.6971 - val_loss: 0.0064 - val_binary_accuracy: 0.9960 - val_precision: 0.8033 - val_recall: 0.5645 - val_f1: 0.6622\n",
      "Epoch 7/10\n",
      "91239/91239 [==============================] - 1090s 12ms/step - loss: 0.0051 - binary_accuracy: 0.9965 - precision: 0.8426 - recall: 0.6228 - f1: 0.7154 - val_loss: 0.0065 - val_binary_accuracy: 0.9959 - val_precision: 0.7753 - val_recall: 0.5945 - val_f1: 0.6721\n",
      "Epoch 8/10\n",
      "91239/91239 [==============================] - 1083s 12ms/step - loss: 0.0048 - binary_accuracy: 0.9967 - precision: 0.8478 - recall: 0.6441 - f1: 0.7312 - val_loss: 0.0065 - val_binary_accuracy: 0.9960 - val_precision: 0.7862 - val_recall: 0.6009 - val_f1: 0.6804\n",
      "Epoch 9/10\n",
      "91239/91239 [==============================] - 1090s 12ms/step - loss: 0.0046 - binary_accuracy: 0.9968 - precision: 0.8511 - recall: 0.6605 - f1: 0.7430 - val_loss: 0.0068 - val_binary_accuracy: 0.9961 - val_precision: 0.8056 - val_recall: 0.5812 - val_f1: 0.6745\n",
      "Epoch 10/10\n",
      "91239/91239 [==============================] - 1090s 12ms/step - loss: 0.0044 - binary_accuracy: 0.9969 - precision: 0.8554 - recall: 0.6767 - f1: 0.7548 - val_loss: 0.0067 - val_binary_accuracy: 0.9960 - val_precision: 0.7819 - val_recall: 0.6004 - val_f1: 0.6785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1485a74e0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics callback causes: AttributeError: 'Model' object has no attribute 'validation_data'\n",
    "# So disable for now\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, \n",
    "    validation_data=(x_val, y_val), \n",
    "    epochs=10, batch_size=128, \n",
    "    callbacks=[tb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 107159 out of 91239 (117.44867874483501)% content items with at least one class assigned at >=0.5 prob\n"
     ]
    }
   ],
   "source": [
    "print(\"There were {} out of {} ({})% content items with at least one class assigned at >=0.5 prob\"\n",
    "      .format(len(np.where(y_pred >= 0.5)[1]), y_pred.shape[0], (len(np.where(y_pred >= 0.5)[1])/y_pred.shape[0])*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred>=P_THRESHOLD] = 1\n",
    "y_pred[y_pred<P_THRESHOLD] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77471105387173311"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ellieking/Documents/tag_tax/govuk-taxonomy-supervised-learning/tax_SL/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.        ,  0.        ,  0.97828233,  0.76923077,  0.        ,\n",
       "         0.86069652,  0.        ,  0.8714653 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.88043478,  0.        ,  0.        ,  0.72852234,\n",
       "         0.        ,  0.68965517,  0.        ,  0.        ,  0.86079295,\n",
       "         0.84431494,  0.8071066 ,  0.82146893,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.87037037,\n",
       "         0.72727273,  0.        ,  0.        ,  0.65740741,  0.91050793,\n",
       "         0.79816514,  0.82506103,  0.71428571,  0.78494624,  0.81345566,\n",
       "         0.        ,  0.89498807,  0.        ,  0.        ,  0.63829787,\n",
       "         0.        ,  0.        ,  0.93835616,  0.91797346,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.91595615,  0.        ,  0.84573503,\n",
       "         1.        ,  0.        ,  0.91666667,  0.88235294,  0.        ,\n",
       "         0.74674115,  0.        ,  1.        ,  0.91666667,  0.        ,\n",
       "         0.        ,  0.        ,  0.83251232,  0.87152778,  0.95288575,\n",
       "         0.86709957,  0.        ,  0.91398431,  0.86075949,  1.        ,\n",
       "         0.87335286,  0.94770793,  0.        ,  0.88176101,  0.        ,\n",
       "         0.        ,  0.8       ,  0.75      ,  0.        ,  0.8944485 ,\n",
       "         0.77672282,  0.75      ,  0.        ,  0.6744186 ,  0.87416332,\n",
       "         0.96494465,  0.6618705 ,  0.        ,  0.6875    ,  0.        ,\n",
       "         0.89519651,  0.        ,  1.        ,  0.92857143,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.93590627,  0.87826087,  0.74736842,\n",
       "         0.88      ,  0.81609195,  0.90830946,  0.9431694 ,  0.        ,\n",
       "         0.88888889,  0.        ,  0.82926829,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.81782845,  0.73076923,\n",
       "         0.86677222,  0.88672238,  0.        ,  0.92592593,  0.        ,\n",
       "         0.        ,  0.        ,  0.94142259,  0.95414847,  0.        ,\n",
       "         0.84293194,  0.825     ,  0.75      ,  0.        ,  0.        ,\n",
       "         0.        ,  0.97884459,  0.        ,  0.        ,  0.92063492,\n",
       "         0.92144177,  0.80962801,  1.        ,  0.        ,  0.78516624,\n",
       "         0.83023544,  0.87429944,  0.77664975,  0.7       ,  0.86448598,\n",
       "         0.94816587,  0.86590321,  0.91005291,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.70033296,  0.73214286,  0.88498403,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.96153846,\n",
       "         0.        ,  0.84210526,  0.9620098 ,  0.        ,  0.7706422 ,\n",
       "         0.90151354,  1.        ,  0.94823529,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.97843666,  0.        ,  0.        ,\n",
       "         0.78775744,  0.        ,  0.69444444,  0.        ,  0.        ,\n",
       "         0.81632653,  1.        ,  0.        ,  0.92522255,  0.88170865,\n",
       "         0.83443709,  0.95522388,  0.        ,  0.80417149,  0.        ,\n",
       "         0.72727273,  0.        ,  0.        ,  1.        ,  0.96491228]),\n",
       " array([ 0.        ,  0.        ,  0.90090909,  0.39473684,  0.        ,\n",
       "         0.33333333,  0.        ,  0.78426836,  0.        ,  0.        ,\n",
       "         0.        ,  0.40298507,  0.        ,  0.        ,  0.55064935,\n",
       "         0.        ,  0.18691589,  0.        ,  0.        ,  0.69537367,\n",
       "         0.76704484,  0.55594406,  0.81274455,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.384     ,  0.54022989,\n",
       "         0.0610687 ,  0.        ,  0.        ,  0.41040462,  0.74022285,\n",
       "         0.37021277,  0.58326143,  0.02673797,  0.52142857,  0.34234234,\n",
       "         0.        ,  0.6432247 ,  0.        ,  0.        ,  0.11363636,\n",
       "         0.        ,  0.        ,  0.65550239,  0.58314176,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.76891616,  0.        ,  0.60284605,\n",
       "         0.00925926,  0.        ,  0.15942029,  0.08356546,  0.        ,\n",
       "         0.58003857,  0.        ,  0.01587302,  0.21854305,  0.        ,\n",
       "         0.        ,  0.        ,  0.53481013,  0.91605839,  0.88318777,\n",
       "         0.77186898,  0.        ,  0.8511335 ,  0.43312102,  0.07894737,\n",
       "         0.86511965,  0.78243626,  0.        ,  0.4056713 ,  0.        ,\n",
       "         0.        ,  0.18181818,  0.49019608,  0.        ,  0.77279899,\n",
       "         0.83807006,  0.83178654,  0.        ,  0.28618421,  0.75930233,\n",
       "         0.70083752,  0.35797665,  0.        ,  0.20625   ,  0.        ,\n",
       "         0.53246753,  0.        ,  0.13043478,  0.34821429,  0.        ,\n",
       "         0.        ,  0.00854701,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.61531491,  0.54891304,  0.34975369,\n",
       "         0.23157895,  0.5503876 ,  0.84646195,  0.90083507,  0.        ,\n",
       "         0.57340242,  0.        ,  0.35051546,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.78196234,  0.28148148,\n",
       "         0.83304348,  0.88099631,  0.        ,  0.33333333,  0.        ,\n",
       "         0.        ,  0.        ,  0.69731405,  0.629683  ,  0.        ,\n",
       "         0.67293626,  0.82978723,  0.08955224,  0.        ,  0.        ,\n",
       "         0.        ,  0.73413344,  0.        ,  0.        ,  0.24166667,\n",
       "         0.91133455,  0.50408719,  0.06060606,  0.        ,  0.61155378,\n",
       "         0.74944072,  0.71984179,  0.43465909,  0.06034483,  0.39956803,\n",
       "         0.87106227,  0.80911288,  0.78989667,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.49412686,  0.20603015,  0.73474801,\n",
       "         0.        ,  0.        ,  0.        ,  0.70833333,  0.42857143,\n",
       "         0.        ,  0.27272727,  0.81685744,  0.        ,  0.45405405,\n",
       "         0.72938944,  0.0862069 ,  0.68074324,  0.08333333,  0.        ,\n",
       "         0.        ,  0.        ,  0.94285714,  0.        ,  0.        ,\n",
       "         0.6568873 ,  0.        ,  0.33783784,  0.        ,  0.        ,\n",
       "         0.42328042,  0.01315789,  0.        ,  0.93859121,  0.66694283,\n",
       "         0.38066465,  0.52459016,  0.        ,  0.83765842,  0.        ,\n",
       "         0.16438356,  0.        ,  0.        ,  0.625     ,  0.92012952]),\n",
       " array([ 0.        ,  0.        ,  0.93800284,  0.52173913,  0.        ,\n",
       "         0.48055556,  0.        ,  0.82557078,  0.        ,  0.        ,\n",
       "         0.        ,  0.55290102,  0.        ,  0.        ,  0.62721893,\n",
       "         0.        ,  0.29411765,  0.        ,  0.        ,  0.76929134,\n",
       "         0.80382721,  0.65838509,  0.81708345,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.55491329,  0.66666667,\n",
       "         0.11267606,  0.        ,  0.        ,  0.50533808,  0.81658231,\n",
       "         0.50581395,  0.68340354,  0.05154639,  0.62660944,  0.48188406,\n",
       "         0.        ,  0.74850299,  0.        ,  0.        ,  0.19292605,\n",
       "         0.        ,  0.        ,  0.77183099,  0.71321462,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.83602001,  0.        ,  0.70392749,\n",
       "         0.01834862,  0.        ,  0.27160494,  0.15267176,  0.        ,\n",
       "         0.65291723,  0.        ,  0.03125   ,  0.35294118,  0.        ,\n",
       "         0.        ,  0.        ,  0.65125241,  0.89323843,  0.91671388,\n",
       "         0.81671764,  0.        ,  0.88143994,  0.57627119,  0.14634146,\n",
       "         0.86921676,  0.85717833,  0.        ,  0.55568767,  0.        ,\n",
       "         0.        ,  0.2962963 ,  0.59288538,  0.        ,  0.82918571,\n",
       "         0.80623112,  0.78877888,  0.        ,  0.40184758,  0.81269446,\n",
       "         0.8119542 ,  0.46464646,  0.        ,  0.31730769,  0.        ,\n",
       "         0.66775244,  0.        ,  0.23076923,  0.50649351,  0.        ,\n",
       "         0.        ,  0.01694915,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.74248223,  0.67558528,  0.47651007,\n",
       "         0.36666667,  0.65740741,  0.87629578,  0.92151628,  0.        ,\n",
       "         0.69711286,  0.        ,  0.49275362,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.79949335,  0.40641711,\n",
       "         0.84957322,  0.88385007,  0.        ,  0.49019608,  0.        ,\n",
       "         0.        ,  0.        ,  0.80118694,  0.75868056,  0.        ,\n",
       "         0.74840209,  0.82738669,  0.16      ,  0.        ,  0.        ,\n",
       "         0.        ,  0.83900965,  0.        ,  0.        ,  0.38283828,\n",
       "         0.91636029,  0.62132662,  0.11428571,  0.        ,  0.68756999,\n",
       "         0.7877719 ,  0.78958785,  0.55737705,  0.11111111,  0.5465288 ,\n",
       "         0.90798015,  0.83654532,  0.84572833,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.57943067,  0.32156863,  0.80289855,\n",
       "         0.        ,  0.        ,  0.        ,  0.82926829,  0.59288538,\n",
       "         0.        ,  0.41201717,  0.88351154,  0.        ,  0.57142857,\n",
       "         0.80636858,  0.15873016,  0.79252704,  0.15384615,  0.        ,\n",
       "         0.        ,  0.        ,  0.96031746,  0.        ,  0.        ,\n",
       "         0.71639462,  0.        ,  0.45454545,  0.        ,  0.        ,\n",
       "         0.55749129,  0.02597403,  0.        ,  0.93185894,  0.75943396,\n",
       "         0.52282158,  0.67724868,  0.        ,  0.82057346,  0.        ,\n",
       "         0.26815642,  0.        ,  0.        ,  0.76923077,  0.94198895]),\n",
       " array([  11,   56, 1100,  152,    9,  519,   96, 1729,    5,   22,   15,\n",
       "         201,   27,   13,  385,    4,  214,   13,    8, 1405, 9255, 1716,\n",
       "        1789,    1,   26,   23,   18,   30,  125,   87,  131,   46,    2,\n",
       "         346, 4577,  235, 3477,  187,  140,  777,    4,  583,   31,   35,\n",
       "         264,   74,    3,  209, 1305,    8,   30,   79,    3,    3,   34,\n",
       "          18,    1,  978,   84, 1546,  108,   42,   69,  359,   11, 2074,\n",
       "          10,   63,  151,    7,   49,   45,  632,  274,  916, 2595,    2,\n",
       "        3970,  157,   76, 1379, 8825,    2, 1728,    6,   37,   44,  153,\n",
       "           9, 3169, 3026,  862,   54,  304,  860, 2985,  257,   20,  160,\n",
       "          57,  385,    8,   46,  112,   19,   38,  117,   49,    3,   17,\n",
       "          11,   47, 2207,  368,  203,  190,  129,  749,  958,   28, 1158,\n",
       "           6,   97,   10,   21,   24,    9,   32, 4036,  135, 4600, 1084,\n",
       "           5,   75,   20,   32,   33,  968,  694,   25,  957,  517,   67,\n",
       "          37,   12,   15, 4916,    2,   12,  240, 1094,  734,  132,   21,\n",
       "         502,  894, 1517,  352,  116,  463, 1365, 2897,  871,   16,    1,\n",
       "          61,   11, 1277,  199, 1131,   31,    2,   43,   72,  175,   58,\n",
       "         176,  961,   54,  185, 5798,   58,  592,   48,   39,   21,   19,\n",
       "         770,   18,    6, 8385,   29,  370,    2,   76,  189,   76,    6,\n",
       "        1661, 1207,  331,  122,    3, 1657,    6,  146,   23,   26,   40,\n",
       "        1853]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average= None, the scores for each class are returned.\n",
    "precision_recall_fscore_support(y_train, y_pred, average=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.87289914986142092, 0.69637885082116113, 0.77471105387173311, None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate globally by counting the total true positives, false negatives and false positives.\n",
    "precision_recall_fscore_support(y_train, y_pred, average='micro', sample_weight=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ellieking/Documents/tag_tax/govuk-taxonomy-supervised-learning/tax_SL/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.47197293041373406, 0.28470325253722589, 0.33016279134606186, None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account\n",
    "precision_recall_fscore_support(y_train, y_pred, average='macro', sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val[y_pred_val>=P_THRESHOLD] = 1\n",
    "y_pred_val[y_pred_val<P_THRESHOLD] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ellieking/Documents/tag_tax/govuk-taxonomy-supervised-learning/tax_SL/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ellieking/Documents/tag_tax/govuk-taxonomy-supervised-learning/tax_SL/lib/python3.4/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.        ,  0.        ,  0.93775934,  0.5       ,  0.        ,\n",
       "         0.6       ,  0.        ,  0.78894472,  0.        ,  0.        ,\n",
       "         0.        ,  0.76470588,  0.        ,  0.        ,  0.48333333,\n",
       "         0.        ,  0.3       ,  0.        ,  0.        ,  0.75      ,\n",
       "         0.75974026,  0.66015625,  0.70379147,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.66666667,\n",
       "         0.5       ,  0.        ,  0.        ,  0.39622642,  0.83957845,\n",
       "         0.51851852,  0.72089041,  1.        ,  0.5       ,  0.46153846,\n",
       "         0.        ,  0.66197183,  0.        ,  0.        ,  0.29411765,\n",
       "         0.        ,  0.        ,  0.85714286,  0.80319149,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.73142857,  0.        ,  0.72727273,\n",
       "         0.        ,  0.        ,  0.        ,  0.625     ,  0.        ,\n",
       "         0.63716814,  0.        ,  1.        ,  0.33333333,  0.        ,\n",
       "         0.        ,  0.        ,  0.70707071,  0.81481481,  0.93548387,\n",
       "         0.79104478,  0.        ,  0.84582441,  0.8125    ,  0.5       ,\n",
       "         0.77456647,  0.8679558 ,  0.        ,  0.74853801,  0.        ,\n",
       "         0.        ,  1.        ,  0.45833333,  0.        ,  0.83579882,\n",
       "         0.68822171,  0.60474308,  0.        ,  0.32      ,  0.70899471,\n",
       "         0.90876565,  0.34285714,  0.        ,  0.45454545,  0.        ,\n",
       "         0.74509804,  0.        ,  1.        ,  0.83333333,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.86031746,  0.71698113,  0.61904762,\n",
       "         0.53333333,  0.71428571,  0.775     ,  0.86010363,  0.        ,\n",
       "         0.75409836,  0.        ,  0.5       ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.73055242,  0.25      ,\n",
       "         0.81776133,  0.77289377,  0.        ,  0.66666667,  0.        ,\n",
       "         0.        ,  0.        ,  0.83663366,  0.87640449,  0.        ,\n",
       "         0.65625   ,  0.69565217,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.94078212,  0.        ,  0.        ,  0.8       ,\n",
       "         0.86101695,  0.45631068,  0.        ,  0.        ,  0.6       ,\n",
       "         0.71717172,  0.73786408,  0.825     ,  1.        ,  0.76086957,\n",
       "         0.88349515,  0.77102804,  0.8313253 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.54867257,  0.5       ,  0.81465517,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.93333333,\n",
       "         0.        ,  0.61538462,  0.90384615,  0.        ,  0.72      ,\n",
       "         0.81888412,  0.        ,  0.88990826,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.91772152,  0.        ,  0.        ,\n",
       "         0.71845238,  0.        ,  0.58974359,  0.        ,  0.        ,\n",
       "         0.6       ,  0.        ,  0.        ,  0.80882353,  0.72197309,\n",
       "         0.51851852,  0.93333333,  0.        ,  0.696     ,  0.        ,\n",
       "         0.5       ,  0.        ,  0.        ,  1.        ,  0.91891892]),\n",
       " array([ 0.        ,  0.        ,  0.79858657,  0.20588235,  0.        ,\n",
       "         0.17910448,  0.        ,  0.64609053,  0.        ,  0.        ,\n",
       "         0.        ,  0.28888889,  0.        ,  0.        ,  0.31521739,\n",
       "         0.        ,  0.10344828,  0.        ,  0.        ,  0.56666667,\n",
       "         0.66531275,  0.39030023,  0.7173913 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.21212121,  0.46153846,\n",
       "         0.06666667,  0.        ,  0.        ,  0.30882353,  0.65300546,\n",
       "         0.25      ,  0.49124854,  0.03846154,  0.17391304,  0.16143498,\n",
       "         0.        ,  0.38842975,  0.        ,  0.        ,  0.08196721,\n",
       "         0.        ,  0.        ,  0.53333333,  0.48242812,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.59534884,  0.        ,  0.52892562,\n",
       "         0.        ,  0.        ,  0.        ,  0.04901961,  0.        ,\n",
       "         0.43813387,  0.        ,  0.08      ,  0.02325581,  0.        ,\n",
       "         0.        ,  0.        ,  0.43209877,  0.86842105,  0.81308411,\n",
       "         0.68831169,  0.        ,  0.75961538,  0.41935484,  0.05555556,\n",
       "         0.77233429,  0.70638489,  0.        ,  0.2962963 ,  0.        ,\n",
       "         0.        ,  0.23076923,  0.32352941,  0.        ,  0.68319226,\n",
       "         0.745     ,  0.68609865,  0.        ,  0.10666667,  0.61187215,\n",
       "         0.63899371,  0.17910448,  0.        ,  0.12820513,  0.        ,\n",
       "         0.48717949,  0.        ,  0.09090909,  0.17857143,  0.        ,\n",
       "         0.        ,  0.03846154,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.51619048,  0.40860215,  0.31707317,\n",
       "         0.16326531,  0.57692308,  0.73372781,  0.69747899,  0.        ,\n",
       "         0.44660194,  0.        ,  0.07407407,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.69379015,  0.05128205,\n",
       "         0.75879828,  0.81153846,  0.        ,  0.21052632,  0.        ,\n",
       "         0.        ,  0.        ,  0.59717314,  0.54545455,  0.        ,\n",
       "         0.59433962,  0.64516129,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.67903226,  0.        ,  0.        ,  0.24615385,\n",
       "         0.8668942 ,  0.27810651,  0.        ,  0.        ,  0.45238095,\n",
       "         0.62008734,  0.58461538,  0.33333333,  0.11111111,  0.30973451,\n",
       "         0.77556818,  0.67715458,  0.61883408,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.37462236,  0.15909091,  0.59810127,\n",
       "         0.        ,  0.        ,  0.        ,  0.54166667,  0.30434783,\n",
       "         0.        ,  0.14545455,  0.71212121,  0.        ,  0.31578947,\n",
       "         0.66666667,  0.        ,  0.58787879,  0.125     ,  0.        ,\n",
       "         0.        ,  0.        ,  0.83815029,  0.        ,  0.        ,\n",
       "         0.56349206,  0.        ,  0.24210526,  0.        ,  0.        ,\n",
       "         0.26086957,  0.        ,  0.        ,  0.84398977,  0.54948805,\n",
       "         0.16666667,  0.51851852,  0.        ,  0.69786096,  0.        ,\n",
       "         0.13888889,  0.        ,  0.        ,  0.5       ,  0.87179487]),\n",
       " array([ 0.        ,  0.        ,  0.86259542,  0.29166667,  0.        ,\n",
       "         0.27586207,  0.        ,  0.71040724,  0.        ,  0.        ,\n",
       "         0.        ,  0.41935484,  0.        ,  0.        ,  0.38157895,\n",
       "         0.        ,  0.15384615,  0.        ,  0.        ,  0.64556962,\n",
       "         0.70939801,  0.49056604,  0.71052632,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.35      ,  0.54545455,\n",
       "         0.11764706,  0.        ,  0.        ,  0.34710744,  0.73463115,\n",
       "         0.3373494 ,  0.58431645,  0.07407407,  0.25806452,  0.23920266,\n",
       "         0.        ,  0.48958333,  0.        ,  0.        ,  0.12820513,\n",
       "         0.        ,  0.        ,  0.65753425,  0.60279441,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.65641026,  0.        ,  0.61244019,\n",
       "         0.        ,  0.        ,  0.        ,  0.09090909,  0.        ,\n",
       "         0.51923077,  0.        ,  0.14814815,  0.04347826,  0.        ,\n",
       "         0.        ,  0.        ,  0.53639847,  0.84076433,  0.87      ,\n",
       "         0.73611111,  0.        ,  0.80040527,  0.55319149,  0.1       ,\n",
       "         0.77344877,  0.77887952,  0.        ,  0.42454395,  0.        ,\n",
       "         0.        ,  0.375     ,  0.37931034,  0.        ,  0.75182967,\n",
       "         0.71548619,  0.64285714,  0.        ,  0.16      ,  0.65686275,\n",
       "         0.75036928,  0.23529412,  0.        ,  0.2       ,  0.        ,\n",
       "         0.58914729,  0.        ,  0.16666667,  0.29411765,  0.        ,\n",
       "         0.        ,  0.07407407,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.6452381 ,  0.52054795,  0.41935484,\n",
       "         0.25      ,  0.63829787,  0.75379939,  0.77030162,  0.        ,\n",
       "         0.56097561,  0.        ,  0.12903226,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.71169687,  0.08510638,\n",
       "         0.7871772 ,  0.79174484,  0.        ,  0.32      ,  0.        ,\n",
       "         0.        ,  0.        ,  0.69690722,  0.67241379,  0.        ,\n",
       "         0.62376238,  0.66945607,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.78875878,  0.        ,  0.        ,  0.37647059,\n",
       "         0.86394558,  0.34558824,  0.        ,  0.        ,  0.5158371 ,\n",
       "         0.66510539,  0.65236052,  0.47482014,  0.2       ,  0.44025157,\n",
       "         0.82602118,  0.7210488 ,  0.70951157,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.44524237,  0.24137931,  0.68978102,\n",
       "         0.        ,  0.        ,  0.        ,  0.7027027 ,  0.45901639,\n",
       "         0.        ,  0.23529412,  0.79661017,  0.        ,  0.43902439,\n",
       "         0.73497689,  0.        ,  0.7080292 ,  0.22222222,  0.        ,\n",
       "         0.        ,  0.        ,  0.87613293,  0.        ,  0.        ,\n",
       "         0.63160649,  0.        ,  0.34328358,  0.        ,  0.        ,\n",
       "         0.36363636,  0.        ,  0.        ,  0.82603254,  0.62403101,\n",
       "         0.25225225,  0.66666667,  0.        ,  0.69692924,  0.        ,\n",
       "         0.2173913 ,  0.        ,  0.        ,  0.66666667,  0.89473684]),\n",
       " array([   1,   13,  283,   34,    7,  134,   23,  486,    1,    7,    1,\n",
       "          45,    2,    2,   92,    1,   58,    0,    3,  360, 2462,  433,\n",
       "         414,    1,    9,    7,    6,    6,   33,   13,   30,   13,    1,\n",
       "          68, 1098,   56,  857,   52,   23,  223,    1,  121,    2,   12,\n",
       "          61,   10,    1,   45,  313,    2,    6,   18,    2,    1,   13,\n",
       "           2,    0,  215,   18,  363,   31,    9,   16,  102,    3,  493,\n",
       "           1,   25,   43,    1,   11,   11,  162,   76,  214,  616,    0,\n",
       "        1040,   31,   18,  347, 2224,    1,  432,    2,   10,   13,   34,\n",
       "           2,  827,  800,  223,   11,   75,  219,  795,   67,    2,   39,\n",
       "          16,   78,    3,   11,   28,    2,   17,   26,    7,    1,    5,\n",
       "           2,    8,  525,   93,   41,   49,   26,  169,  238,    5,  309,\n",
       "           1,   27,    2,    1,    9,    2,    7,  934,   39, 1165,  260,\n",
       "           2,   19,    2,    6,    7,  283,  143,    8,  212,  124,   13,\n",
       "          10,    4,    4, 1240,    0,    3,   65,  293,  169,   32,   17,\n",
       "         126,  229,  390,   99,   27,  113,  352,  731,  223,    3,    1,\n",
       "          14,    5,  331,   44,  316,   11,    1,   18,   24,   46,    9,\n",
       "          55,  264,   17,   57, 1431,   11,  165,   16,    8,    5,    6,\n",
       "         173,    7,    3, 2142,    5,   95,    3,   13,   46,   26,    3,\n",
       "         391,  293,   84,   27,    4,  374,    2,   36,    4,    8,    8,\n",
       "         468]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average= None, the scores for each class are returned.\n",
    "precision_recall_fscore_support(y_val, y_pred_val, average=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.78140344094335978, 0.59946611300608033, 0.67844914400805634, None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate globally by counting the total true positives, false negatives and false positives.\n",
    "precision_recall_fscore_support(y_val, y_pred_val, average='micro', sample_weight=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ellieking/Documents/tag_tax/govuk-taxonomy-supervised-learning/tax_SL/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ellieking/Documents/tag_tax/govuk-taxonomy-supervised-learning/tax_SL/lib/python3.4/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.37480943520284948, 0.22834536637549022, 0.26643755724626694, None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account\n",
    "precision_recall_fscore_support(y_val, y_pred_val, average='macro', sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve accuracy by training longer with some regularization mechanism (such as dropout) or by fine-tuning the Embedding layer.\n",
    "\n",
    "We can also test how well we would have performed by not using pre-trained word embeddings, but instead initializing our Embedding layer from scratch and learning its weights during training. We just need to replace our Embedding layer with the following:\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)\n",
    "                            \n",
    "After 2 epochs, this approach only gets us to 90% validation accuracy, less than what the previous model could reach in just one epoch. Our pre-trained embeddings were definitely buying us something. In general, using pre-trained embeddings is relevant for natural processing tasks were little training data is available (functionally the embeddings act as an injection of outside information which might prove useful for your model).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a classifier optimized for maximizing f1_score (uses class_weights)\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "# clf = Sequential()\n",
    "\n",
    "# clf.add(Dense(x_train.shape[1], 1600, activation='relu'))\n",
    "# clf.add(Dropout(0.6))\n",
    "# clf.add(Dense(1600, 1200, activation='relu'))\n",
    "# clf.add(Dropout(0.6))\n",
    "# clf.add(Dense(1200, 800, activation='relu'))\n",
    "# clf.add(Dropout(0.6))\n",
    "# clf.add(Dense(800, y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "# clf.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "\n",
    "# clf.fit(xt, yt, batch_size=64, nb_epoch=300, validation_data=(xs, ys), class_weight=W, verbose=0)\n",
    "\n",
    "# preds = clf.predict(xs)\n",
    "\n",
    "# preds[preds>=0.5] = 1\n",
    "# preds[preds<0.5] = 0\n",
    "\n",
    "# print(f1_score(ys, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "keep_output": true,
  "kernelspec": {
   "display_name": "tax_SL",
   "language": "python",
   "name": "tax_sl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
